# To support both python 2 and python 3
from __future__ import division, print_function, unicode_literals

# Common imports
import numpy as np
import os
import tarfile # 압축파일 라이브러리
from six.moves import urllib # 데이터의 위치를 알려주는 라이브러리
import pandas as pd # 엑셀형식 데이터 처리 모듈

# to make this notebook's output stable across runs
np.random.seed(42)

# To plot pretty figures
import matplotlib as mpl
import matplotlib.pyplot as plt

from sklearn.model_selection import train_test_split       # 테스트와 훈련용 케이스를 분류
from sklearn.model_selection import StratifiedShuffleSplit # Stratified로 테스트 케이스를 분류
from sklearn.impute import SimpleImputer # 데이터 조작 모듈
from sklearn.preprocessing import OrdinalEncoder # 숫자 순서대로 인코딩해주는 모듈
from sklearn.base import BaseEstimator, TransformerMixin
from sklearn.preprocessing import FunctionTransformer
from sklearn.pipeline import Pipeline
from sklearn.preprocessing import StandardScaler
from sklearn.preprocessing import OneHotEncoder
from sklearn.compose import ColumnTransformer
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error
from sklearn.tree import DecisionTreeRegressor
from sklearn.ensemble import RandomForestRegressor
from sklearn.svm import SVR
from sklearn.model_selection import cross_val_score

# Ignore useless warnings (see SciPy issue #5998)
import warnings
warnings.filterwarnings(action="ignore", message="^internal gelsd")

# Where to save the figures
PROJECT_ROOT_DIR = "."
CHAPTER_ID = "end_to_end_project"
IMAGES_PATH = os.path.join(PROJECT_ROOT_DIR, "images", CHAPTER_ID)

DOWNLOAD_ROOT = "https://raw.githubusercontent.com/ageron/handson-ml/master/"
HOUSING_PATH = os.path.join("datasets", "housing")
HOUSING_FILE = "housing.tgz"
HOUSING_URL = DOWNLOAD_ROOT + "datasets/housing/housing.tgz"

def set_customize_plot_rc():
    mpl.rc('axes', labelsize=14)
    mpl.rc('xtick', labelsize=12)
    mpl.rc('ytick', labelsize=12)

def save_fig(fig_id, tight_layout=True, fig_extension="png", resolution=300):
    path = os.path.join(IMAGES_PATH, fig_id + "." + fig_extension)
    print("Saving figure", fig_id)
    if tight_layout:
        plt.tight_layout()
    plt.savefig(path, format=fig_extension, dpi=resolution)

def fetch_housing_data(housing_url=HOUSING_URL, housing_path=HOUSING_PATH):
    if not os.path.isdir(housing_path): # 장소가 이미 있다면
        os.makedirs(housing_path) # 만들지 않음
    tgz_path = os.path.join(housing_path, "housing.tgz") # 확장자 이름과 경로 생성
    urllib.request.urlretrieve(housing_url, tgz_path)
    housing_tgz = tarfile.open(tgz_path) # 압축 파일 오픈
    housing_tgz.extractall(path=housing_path) # 압축 해제
    housing_tgz.close() # 압축 파일 닫기

def load_housing_data(housing_path=HOUSING_PATH):
    csv_path = os.path.join(housing_path, "housing.csv")
    return pd.read_csv(csv_path)

def insight_data(): # 데이터 들을 점검하는 함수
    print(housing.head()) # 데어터 출력
    print(housing.info()) # 데이터 정보 출력
    print(housing["ocean_proximity"].value_counts()) # ocean_proximity에 대한 정보 출력
    print(housing.describe()) # 개수, 평균, 기준 값, 표준 편차 등 데이터 속성 출력
    if not os.path.isdir(IMAGES_PATH): # 장소가 이미 있다면
        os.makedirs(IMAGES_PATH) # 만들지 않음
    housing.hist(bins=50, figsize=(20,15)) # 히스토 그램을 그림(막대기 수, 그림 사이즈)
    save_fig("attribute_histogram_plots") # 위에 정의해 놓은 함수 호출
    plt.show()

def split_train_test(data, test_ratio): 
    shuffled_indices = np.random.permutation(len(data)) # 데이터를 셔플
    test_set_size = int(len(data) * test_ratio)
    test_indices = shuffled_indices[:test_set_size] # 테스트용: 데이터 들을 구분 (0~사이즈 까지)
    train_indices = shuffled_indices[test_set_size:]# 학습용: 데이터 들을 구분 (사이즈~끝까지)
    return data.iloc[train_indices], data.iloc[test_indices] # 테스트용, 학습용으로 데이터를 잘라 줌

class CombinedAttributesAdder(BaseEstimator, TransformerMixin):
    def __init__(self, add_bedrooms_per_room = True): # no *args or **kwargs
        self.add_bedrooms_per_room = add_bedrooms_per_room
    def fit(self, X, y=None):
        return self  # nothing else to do
    def transform(self, X, y=None):
        rooms_per_household = X[:, rooms_ix] / X[:, household_ix]
        population_per_household = X[:, population_ix] / X[:, household_ix]
        if self.add_bedrooms_per_room:
            bedrooms_per_room = X[:, bedrooms_ix] / X[:, rooms_ix]
            return np.c_[X, rooms_per_household, population_per_household,
                         bedrooms_per_room]
        else:
            return np.c_[X, rooms_per_household, population_per_household]

def add_extra_features(X, add_bedrooms_per_room=True):
    rooms_per_household = X[:, rooms_ix] / X[:, household_ix]
    population_per_household = X[:, population_ix] / X[:, household_ix]
    if add_bedrooms_per_room:
        bedrooms_per_room = X[:, bedrooms_ix] / X[:, rooms_ix]
        return np.c_[X, rooms_per_household, population_per_household,
                     bedrooms_per_room]
    else:
        return np.c_[X, rooms_per_household, population_per_household]

def linearregression(housing_prepared,housing_labels):
    lin_reg = LinearRegression()
    lin_reg.fit(housing_prepared, housing_labels)

    lin_scores = cross_val_score(lin_reg, housing_prepared, housing_labels,
                                 scoring="neg_mean_squared_error", cv=10)
    lin_rmse_scores = np.sqrt(-lin_scores)
    print("linearregression = ",lin_rmse_scores.mean())

def decisiontreeregressor(housing_prepared,housing_labels):
    tree_reg = DecisionTreeRegressor(random_state=42)
    tree_reg.fit(housing_prepared, housing_labels)

    scores = cross_val_score(tree_reg, housing_prepared, housing_labels,
                         scoring="neg_mean_squared_error", cv=10)
    tree_rmse_scores = np.sqrt(-scores)
    print("decisiontreeregressor = ",tree_rmse_scores.mean())

def randomforestregressor(housing_prepared,housing_labels):
    forest_reg = RandomForestRegressor(n_estimators=10, random_state=42)
    forest_reg.fit(housing_prepared, housing_labels)

    forest_scores = cross_val_score(forest_reg, housing_prepared, housing_labels,
                                scoring="neg_mean_squared_error", cv=10)
    forest_rmse_scores = np.sqrt(-forest_scores)
    print("randomforestregressor = ",forest_rmse_scores.mean())

def svr(housing_prepared,housing_labels):
    svm_reg = SVR(kernel="linear")
    svm_reg.fit(housing_prepared, housing_labels)

    svm_scores = cross_val_score(svm_reg, housing_prepared, housing_labels,
                                scoring="neg_mean_squared_error", cv=10)
    svm_rmse_scores = np.sqrt(-svm_scores)
    print("svr = ",svm_rmse_scores.mean())


def First():
    set_customize_plot_rc()
    if not os.path.isfile(os.path.join(HOUSING_PATH, HOUSING_FILE)): # 압축 파일 있는지 확인
        fetch_housing_data()
    housing = load_housing_data() # 데이터 불러오기
    
    train_set, test_set = train_test_split(housing,
                                           test_size=0.2,
                                           random_state=42) # 항상 일정한 값을 가져오기 위해 랜덤 부여

    housing["income_cat"] = np.ceil(housing["median_income"] / 1.5) # 분류를 위해 income 카테고리를 새로 만듦 
    #print(housing.head())

    housing["income_cat"] = pd.cut(housing["median_income"], # "median_income"에 대한 새로운 카테고리 생성
                               bins=[0., 1.5, 3.0, 4.5, 6., np.inf], # 기준을 나눔
                               labels=[1, 2, 3, 4, 5]) # 기준에 따라 라벨링



    split = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=42) # 학습용, 테스트용 스플릿
    for train_index, test_index in split.split(housing, housing["income_cat"]):
        strat_train_set = housing.loc[train_index]
        strat_test_set = housing.loc[test_index]

    strat_test_set["income_cat"].value_counts() / len(strat_test_set) # 전체에대한 income_cat의 비율

    for set_ in (strat_train_set, strat_test_set): # 적절히 잘 나누었으므로
        set_.drop("income_cat", axis=1, inplace=True) # income_cat 제거(drop)

    housing = strat_train_set.copy() # train된 데이터 셋을 housing으로 정의 (하드카피)


    # X:기계 학습의 입력 데이터
    housing = strat_train_set.drop("median_house_value", axis=1) # median_house_value이라는 출력을 알고 싶기 때문에 housing에서 삭제
    # Y:기계 학습의 출력 데이터
    housing_labels = strat_train_set["median_house_value"].copy() # 출력


    housing_cat = housing[['ocean_proximity']]
    

    rooms_ix, bedrooms_ix, population_ix, household_ix = [
    list(housing.columns).index(col)
    for col in ("total_rooms", "total_bedrooms", "population", "households")]    # 위에 주석 작업을 한 번에    
    housing_num = housing.drop('ocean_proximity', axis=1) # housing에서 숫자가 있는것만 뽑기
    num_pipeline = Pipeline([
        ('imputer', SimpleImputer(strategy="median")), # median으로 설정했으므로 중간 값만 뽑기
        #('attribs_adder', FunctionTransformer(add_extra_features, validate=False)),
        #('std_scaler', StandardScaler()),
    ])
    
    num_attribs = list(housing_num)
    cat_attribs = ["ocean_proximity"]

    full_pipeline = ColumnTransformer([
            ("num", num_pipeline, num_attribs),
            ("cat", OneHotEncoder(), cat_attribs),
        ])

    
    housing_prepared = full_pipeline.fit_transform(housing)

    print("Housing data에서 어떤 feature도 추가하지 않은값")
    linearregression(housing_prepared,housing_labels)
    decisiontreeregressor(housing_prepared,housing_labels)
    randomforestregressor(housing_prepared,housing_labels)
    svr(housing_prepared,housing_labels)

def Second():
   print() 
def Third():
    print()
    
if __name__ == '__main__':
    First()
    #Second()
    #Third()
    
    set_customize_plot_rc()
    if not os.path.isfile(os.path.join(HOUSING_PATH, HOUSING_FILE)): # 압축 파일 있는지 확인
        fetch_housing_data()
    housing = load_housing_data() # 데이터 불러오기
    
    train_set, test_set = train_test_split(housing,
                                           test_size=0.2,
                                           random_state=42) # 항상 일정한 값을 가져오기 위해 랜덤 부여

    housing["income_cat"] = np.ceil(housing["median_income"] / 1.5) # 분류를 위해 income 카테고리를 새로 만듦 
    
    housing["income_cat"] = pd.cut(housing["median_income"], # "median_income"에 대한 새로운 카테고리 생성
                               bins=[0., 1.5, 3.0, 4.5, 6., np.inf], # 기준을 나눔
                               labels=[1, 2, 3, 4, 5]) # 기준에 따라 라벨링



    split = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=42) # 학습용, 테스트용 스플릿
    for train_index, test_index in split.split(housing, housing["income_cat"]):
        strat_train_set = housing.loc[train_index]
        strat_test_set = housing.loc[test_index]

    strat_test_set["income_cat"].value_counts() / len(strat_test_set) # 전체에대한 income_cat의 비율

    for set_ in (strat_train_set, strat_test_set): # 적절히 잘 나누었으므로
        set_.drop("income_cat", axis=1, inplace=True) # income_cat 제거(drop)

    housing = strat_train_set.copy() # train된 데이터 셋을 housing으로 정의 (하드카피)


    # X:기계 학습의 입력 데이터
    housing = strat_train_set.drop("median_house_value", axis=1) # median_house_value이라는 출력을 알고 싶기 때문에 housing에서 삭제
    # Y:기계 학습의 출력 데이터
    housing_labels = strat_train_set["median_house_value"].copy() # 출력


    housing_cat = housing[['ocean_proximity']]
    

    rooms_ix, bedrooms_ix, population_ix, household_ix = [
    list(housing.columns).index(col)
    for col in ("total_rooms", "total_bedrooms", "population", "households")]    # 위에 주석 작업을 한 번에    
    housing_num = housing.drop('ocean_proximity', axis=1) # housing에서 숫자가 있는것만 뽑기
    num_pipeline = Pipeline([
        ('imputer', SimpleImputer(strategy="median")), # median으로 설정했으므로 중간 값만 뽑기
        ('attribs_adder', FunctionTransformer(add_extra_features, validate=False)),
        #('std_scaler', StandardScaler()),
    ])
    
    num_attribs = list(housing_num)
    cat_attribs = ["ocean_proximity"]

    full_pipeline = ColumnTransformer([
            ("num", num_pipeline, num_attribs),
            ("cat", OneHotEncoder(), cat_attribs),
        ])

    
    housing_prepared = full_pipeline.fit_transform(housing)
    print()
    print()
    print("자질을 추가한 값")
    linearregression(housing_prepared,housing_labels)
    decisiontreeregressor(housing_prepared,housing_labels)
    randomforestregressor(housing_prepared,housing_labels)
    svr(housing_prepared,housing_labels)
    
    
#################################################################################################################    

    set_customize_plot_rc()
    if not os.path.isfile(os.path.join(HOUSING_PATH, HOUSING_FILE)): # 압축 파일 있는지 확인
        fetch_housing_data()
    housing = load_housing_data() # 데이터 불러오기
    
    train_set, test_set = train_test_split(housing,
                                           test_size=0.2,
                                           random_state=42) # 항상 일정한 값을 가져오기 위해 랜덤 부여

    housing["income_cat"] = np.ceil(housing["median_income"] / 1.5) # 분류를 위해 income 카테고리를 새로 만듦 
    
    housing["income_cat"] = pd.cut(housing["median_income"], # "median_income"에 대한 새로운 카테고리 생성
                               bins=[0., 1.5, 3.0, 4.5, 6., np.inf], # 기준을 나눔
                               labels=[1, 2, 3, 4, 5]) # 기준에 따라 라벨링



    split = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=42) # 학습용, 테스트용 스플릿
    for train_index, test_index in split.split(housing, housing["income_cat"]):
        strat_train_set = housing.loc[train_index]
        strat_test_set = housing.loc[test_index]

    strat_test_set["income_cat"].value_counts() / len(strat_test_set) # 전체에대한 income_cat의 비율

    for set_ in (strat_train_set, strat_test_set): # 적절히 잘 나누었으므로
        set_.drop("income_cat", axis=1, inplace=True) # income_cat 제거(drop)

    housing = strat_train_set.copy() # train된 데이터 셋을 housing으로 정의 (하드카피)


    # X:기계 학습의 입력 데이터
    housing = strat_train_set.drop("median_house_value", axis=1) # median_house_value이라는 출력을 알고 싶기 때문에 housing에서 삭제
    # Y:기계 학습의 출력 데이터
    housing_labels = strat_train_set["median_house_value"].copy() # 출력


    housing_cat = housing[['ocean_proximity']]
    

    rooms_ix, bedrooms_ix, population_ix, household_ix = [
    list(housing.columns).index(col)
    for col in ("total_rooms", "total_bedrooms", "population", "households")]    # 위에 주석 작업을 한 번에    
    housing_num = housing.drop('ocean_proximity', axis=1) # housing에서 숫자가 있는것만 뽑기
    num_pipeline = Pipeline([
        ('imputer', SimpleImputer(strategy="median")), # median으로 설정했으므로 중간 값만 뽑기
        ('attribs_adder', FunctionTransformer(add_extra_features, validate=False)),
        ('std_scaler', StandardScaler()),
    ])
    
    num_attribs = list(housing_num)
    cat_attribs = ["ocean_proximity"]

    full_pipeline = ColumnTransformer([
            ("num", num_pipeline, num_attribs),
            ("cat", OneHotEncoder(), cat_attribs),
        ])

    
    housing_prepared = full_pipeline.fit_transform(housing)

    print()
    print()
    print("2)의 결과에서 StandardScaler을 사용한 값")
    
    linearregression(housing_prepared,housing_labels)
    decisiontreeregressor(housing_prepared,housing_labels)
    randomforestregressor(housing_prepared,housing_labels)
    svr(housing_prepared,housing_labels)   
    
    
